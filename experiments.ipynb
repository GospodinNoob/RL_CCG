{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'env')\n",
    "\n",
    "import ccg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "seed = 12345644\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "cards = pd.read_csv('env/configue/cardsTable.csv')\n",
    "cardsList = [ccg.Minion(i) for i in cards.values.tolist()]\n",
    "\n",
    "cores = pd.read_csv('env/configue/cores.csv')\n",
    "coreList = [ccg.Core(i) for i in cores.values.tolist()]\n",
    "\n",
    "playersNum = 2\n",
    "piles = [[ccg.Pile(cardsList, 10) for _ in range(4)] for I in range(playersNum)]\n",
    "cores = np.random.choice(coreList, 2)\n",
    "decks = [ccg.Deck(cores[i], piles[i], i) for i in range(playersNum)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = ccg.Session(decks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observationMinion(minion):\n",
    "    state = list(minion)\n",
    "    state[5] = int(state[5])\n",
    "    return np.array(state)\n",
    "\n",
    "CARD_SIZE = len(observationMinion(ccg.Minion().getCurState()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observationTable(table, turn):\n",
    "    tables = copy.deepcopy(table)\n",
    "    for i in tables:\n",
    "        #TODO : don't calc empty minions\n",
    "        for j in range(len(i), 8):\n",
    "            i.append(np.array([-1] * CARD_SIZE))\n",
    "        i[0] = observationCore(i[0])\n",
    "        for j in range(1, len(i)):\n",
    "            i[j] = observationMinion(i[j])\n",
    "        buf = i[1:]\n",
    "        #random.shuffle(buf)\n",
    "        i[1:] = buf\n",
    "    tables[0], tables[turn] = tables[turn], tables[0]\n",
    "    tables_copy = copy.deepcopy(tables)\n",
    "    \n",
    "    for i in range(len(tables)):\n",
    "        tables[i] = np.hstack(tuple(tables[i]))\n",
    "        for j in range(len(tables_copy[i])):\n",
    "            tables_copy[i][j] = tables_copy[i][j].tolist()\n",
    "    return np.hstack(tuple(tables)), tables_copy\n",
    "\n",
    "def observationCore(core):\n",
    "    return np.array(list(core))\n",
    "\n",
    "def observationPile(pile):\n",
    "    pileCopy = copy.deepcopy(pile)\n",
    "    #random.shuffle(pileCopy)\n",
    "    for i in range(len(pileCopy)):\n",
    "        pileCopy[i] = list(pileCopy[i])\n",
    "        if pileCopy[i][1] == None:\n",
    "            pileCopy[i][1] = np.array([-1] * CARD_SIZE)\n",
    "        else:\n",
    "            pileCopy[i][1] = observationMinion(pileCopy[i][1])\n",
    "        pileCopy[i] = [pileCopy[i][0]] + pileCopy[i][1].tolist()\n",
    "    return np.hstack(tuple(pileCopy)), pileCopy\n",
    "\n",
    "def observationHand(hand):\n",
    "    handCopy = copy.deepcopy(hand)\n",
    "    #random.shuffle(pileCopy)\n",
    "    for i in range(len(hand[1]), 6):\n",
    "        handCopy[1].append(None)\n",
    "    for i in range(len(handCopy[1])):\n",
    "        if handCopy[1][i] == None:\n",
    "            handCopy[1][i] = np.array([-1] * CARD_SIZE)\n",
    "        else:\n",
    "            handCopy[1][i] = observationMinion(handCopy[1][i])\n",
    "    return [handCopy[0]] + np.hstack(tuple(handCopy[1])), handCopy[1]\n",
    "\n",
    "def createStateObservation(state):\n",
    "    \n",
    "    observations = dict()\n",
    "    observations[\"table\"], tables = observationTable(state[\"battleGround\"][\"table\"], state[\"turn\"])\n",
    "    observations[\"piles\"] = []\n",
    "    observations[\"hands\"] = []\n",
    "    pilesObs = []\n",
    "    handsObs = []\n",
    "    \n",
    "    for i in state[\"piles\"]:\n",
    "        obs, obj = observationPile(i)\n",
    "        observations[\"piles\"].append(obj)\n",
    "        pilesObs.append(obs)\n",
    "        \n",
    "    for i in state[\"hands\"]:\n",
    "        obs, obj = observationHand(i)\n",
    "        observations[\"hands\"].append(obj)\n",
    "        handsObs.append(obs)\n",
    "    observations[\"hands\"][0], observations[\"hands\"][state[\"turn\"]] = observations[\"hands\"][state[\"turn\"]], observations[\"hands\"][0]\n",
    "    handsObs[0], handsObs[state[\"turn\"]] = handsObs[state[\"turn\"]], handsObs[0]\n",
    "    handsObs = np.hstack(tuple(handsObs))\n",
    "    \n",
    "    observations[\"piles\"][0], observations[\"piles\"][state[\"turn\"]] = observations[\"piles\"][state[\"turn\"]], observations[\"piles\"][0]\n",
    "    pilesObs[0], pilesObs[state[\"turn\"]] = pilesObs[state[\"turn\"]], pilesObs[0]\n",
    "    pilesObs = np.hstack(tuple(pilesObs))\n",
    "    \n",
    "    observations[\"main\"] = observations[\"table\"].tolist() + pilesObs.tolist() + handsObs.tolist()\n",
    "    \n",
    "    cores = []\n",
    "    units = []\n",
    "    for i in tables:\n",
    "        cores.append(i[0])\n",
    "        units.append(i[1:])\n",
    "\n",
    "    observations[\"cores\"] = cores\n",
    "    observations[\"units\"] = units\n",
    "    \n",
    "    return observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2732e876f497>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshould_explore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateStateObservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[0mMAIN_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"main\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[0mCARD_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservationMinion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mccg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMinion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetCurState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-e66a18d8e2bb>\u001b[0m in \u001b[0;36mcreateStateObservation\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mobservations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mobservations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"table\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservationTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"battleGround\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"table\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"turn\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[0mobservations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"piles\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mobservations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"hands\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, MAIN_SIZE, CARD_SIZE, CORE_SIZE, PILE_SIZE, HAND_SIZE, VEC_SIZE, epsilon = 0.5):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.field2vec = nn.Sequential(\n",
    "            nn.Linear(MAIN_SIZE, 512,), \n",
    "            nn.ELU(), \n",
    "            nn.Linear(512, VEC_SIZE))\n",
    "        \n",
    "        self.skip_qvalue = nn.Sequential(\n",
    "            nn.Linear(MAIN_SIZE, 512,), \n",
    "            nn.ELU(), \n",
    "            nn.Linear(512, 1))\n",
    "        \n",
    "        self.card2vec = nn.Sequential(\n",
    "            nn.Linear(CARD_SIZE, 512,), \n",
    "            nn.ELU(), \n",
    "            nn.Linear(512, VEC_SIZE))\n",
    "        \n",
    "        self.attack_units_qvalues = nn.Sequential(\n",
    "            nn.Linear(VEC_SIZE, 512,),  #[field, attacker_card, target_card]\n",
    "            nn.ELU(), \n",
    "            nn.Linear(512, 1))\n",
    "        \n",
    "        self.core2vec = nn.Sequential(\n",
    "            nn.Linear(CORE_SIZE, 512,), \n",
    "            nn.ELU(), \n",
    "            nn.Linear(512, VEC_SIZE))\n",
    "        \n",
    "        self.pile2vec = nn.Sequential(\n",
    "            nn.Linear(PILE_SIZE, 512,), \n",
    "            nn.ELU(), \n",
    "            nn.Linear(512, VEC_SIZE))\n",
    "        \n",
    "        self.hand2vec = nn.Sequential(\n",
    "            nn.Linear(HAND_SIZE, 512,), \n",
    "            nn.ELU(), \n",
    "            nn.Linear(512, VEC_SIZE))\n",
    "        \n",
    "        self.attack_core_qvalues = nn.Sequential(\n",
    "            nn.Linear(VEC_SIZE, 512,),  #[field, attacker_card, core]\n",
    "            nn.ELU(), \n",
    "            nn.Linear(512, 1))\n",
    "        \n",
    "        self.play_card_qvalues = nn.Sequential(\n",
    "            nn.Linear(VEC_SIZE, 512,),  #[field, pile]\n",
    "            nn.ELU(), \n",
    "            nn.Linear(512, 1))\n",
    "        \n",
    "        self.play_hand_card_qvalues = nn.Sequential(\n",
    "            nn.Linear(VEC_SIZE, 512,),  #[field, hand]\n",
    "            nn.ELU(), \n",
    "            nn.Linear(512, 1))\n",
    "        \n",
    "        self.move_card_qvalues = nn.Sequential(\n",
    "            nn.Linear(VEC_SIZE, 512,),  #[field, pile]\n",
    "            nn.ELU(), \n",
    "            nn.Linear(512, 1))\n",
    "    \n",
    "    def parse_state(self, state):\n",
    "        main = np.array(state[\"main\"], dtype=np.float32)[None, None, None, :]\n",
    "        our_units = np.array(state[\"units\"][0], dtype=np.float32)[None, :, None, :]\n",
    "        enemy_units = np.array(state[\"units\"][1], dtype=np.float32)[None, None, :, :]\n",
    "        enemy_core = np.array([state[\"cores\"][1]], dtype=np.float32)[None, None, :]\n",
    "        our_piles = np.array(state[\"piles\"][0], dtype=np.float32)[None, :, None, :]\n",
    "        our_hand = np.array(state[\"hands\"][0], dtype=np.float32)[None, None, :]\n",
    "        return main, our_units, enemy_units, enemy_core, our_piles, our_hand\n",
    "    \n",
    "    def get_qvalues_from_state(self, state):\n",
    "        main, our_units, enemy_units, enemy_core, our_piles, our_hand = self.parse_state(state)\n",
    "        main = np.array([main])\n",
    "        enemy_units = np.array([enemy_units])\n",
    "        enemy_core = np.array([enemy_core])\n",
    "        our_piles = np.array([our_piles])\n",
    "        our_units = np.array([our_units])\n",
    "        our_hand = np.array([our_hand])\n",
    "        return self.forward(main, our_units, enemy_units, enemy_core, our_piles, our_hand)\n",
    "        \n",
    "    def forward(self, main, our_units, enemy_units, enemy_core, our_piles, our_hand):\n",
    "        \n",
    "        qvalues = self.compute_qvalue(\n",
    "            torch.from_numpy(main),\n",
    "            torch.from_numpy(our_units),\n",
    "            torch.from_numpy(enemy_units),\n",
    "            torch.from_numpy(enemy_core),\n",
    "            torch.from_numpy(our_piles),\n",
    "            torch.from_numpy(our_hand)\n",
    "        )\n",
    "        \n",
    "        return qvalues\n",
    "    \n",
    "    def record():\n",
    "        pass\n",
    "    \n",
    "    def compute_qvalue(self, field, card, target_card, core, pile, hand):\n",
    "        field_vec = self.field2vec(field)\n",
    "        card_vec = self.card2vec(card)\n",
    "        target_vec = self.card2vec(target_card)\n",
    "        pile_vec = self.pile2vec(pile)\n",
    "        core_vec = self.core2vec(core)\n",
    "        hand_vec = self.hand2vec(hand)\n",
    "        \n",
    "        batch_size = len(core)\n",
    "        #print(hand_vec.shape)\n",
    "        #print(field_vec.shape, card_vec.shape, target_vec.shape)\n",
    "        attack_units_qvalue = self.attack_units_qvalues(field_vec + card_vec + target_vec)\n",
    "        #print(attack_units_qvalue.shape)\n",
    "        attack_units_qvalue = attack_units_qvalue.view(-1)\n",
    "        attack_units_qvalue = attack_units_qvalue.view(batch_size, len(attack_units_qvalue) // batch_size)\n",
    "        #print(attack_units_qvalue.shape[1] == 49)\n",
    "        \n",
    "        #print(field_vec.shape, card_vec.shape, core_vec.shape)\n",
    "        attack_core_qvalue = self.attack_core_qvalues(field_vec + card_vec + core_vec)\n",
    "        #print(attack_core_qvalue.shape)\n",
    "        attack_core_qvalue = attack_core_qvalue.view(-1)\n",
    "        attack_core_qvalue = attack_core_qvalue.view(batch_size, len(attack_core_qvalue) // batch_size)\n",
    "        #print(attack_core_qvalue.shape[1] == 7)\n",
    "        \n",
    "        play_card_qvalue = self.play_card_qvalues(field_vec + pile_vec).view(-1)\n",
    "        play_card_qvalue = play_card_qvalue.view(batch_size, len(play_card_qvalue) // batch_size)\n",
    "        #print(play_card_qvalue.shape[1] == 4)\n",
    "        \n",
    "        skip_qvalue = self.skip_qvalue(field).view(-1)\n",
    "        skip_qvalue = skip_qvalue.view(batch_size, len(skip_qvalue) // batch_size)\n",
    "        #print(skip_qvalue.shape[1] == 1)\n",
    "        \n",
    "        play_hand_card_qvalue = self.play_hand_card_qvalues(field_vec + hand_vec).view(-1)\n",
    "        play_hand_card_qvalue = play_hand_card_qvalue.view(batch_size, len(play_hand_card_qvalue) // batch_size)\n",
    "        #print(play_hand_card_qvalue.shape[1])\n",
    "        \n",
    "        move_card_qvalue = self.move_card_qvalues(field_vec + pile_vec).view(-1)\n",
    "        move_card_qvalue = move_card_qvalue.view(batch_size, len(move_card_qvalue) // batch_size)\n",
    "        #print(move_card_qvalue.shape[1])\n",
    "        \n",
    "        return torch.cat((skip_qvalue, \n",
    "                          attack_core_qvalue, \n",
    "                          attack_units_qvalue, \n",
    "                          play_card_qvalue, \n",
    "                          play_hand_card_qvalue,\n",
    "                          move_card_qvalue\n",
    "                         ), dim=1)\n",
    "    \n",
    "    def sample_actions(self, qvalues, valid_actions):\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "        \n",
    "        #print(qvalues.shape, valid_actions.shape)\n",
    "        \n",
    "        qvalues[np.logical_not(valid_actions)] = -2**32\n",
    "        valid_actions = valid_actions.astype(np.int)\n",
    "        valid_actions = [va / np.sum(va) for va in valid_actions]\n",
    "        random_actions = [np.random.choice(n_actions, size=batch_size, p=va)[0] for va in valid_actions]\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "        \n",
    "        should_explore = np.random.choice([0, 1], batch_size, p = [1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)\n",
    "\n",
    "state = createStateObservation(session.reset())\n",
    "MAIN_SIZE = len(state[\"main\"])\n",
    "CARD_SIZE = len(observationMinion(ccg.Minion().getCurState()))\n",
    "CORE_SIZE = len(state[\"cores\"][0])\n",
    "PILE_SIZE = len(state[\"piles\"][0][0])\n",
    "HAND_SIZE = len(state[\"hands\"][0])\n",
    "VEC_SIZE = 100\n",
    "\n",
    "actor_network = ActorNetwork(MAIN_SIZE, CARD_SIZE, CORE_SIZE, PILE_SIZE, HAND_SIZE, VEC_SIZE)\n",
    "actor_network_optim = torch.optim.Adam(actor_network.parameters(), lr = 0.01)\n",
    "\n",
    "#[FIELD_INDEX, OUR_CARD_INDEX, TARGET_CARD_INDEX, VEC]\n",
    "main, our_units, enemy_units, enemy_core, our_piles, our_hand = actor_network.parse_state(state)\n",
    "qvalues = actor_network(np.array([main]),\n",
    "                                 np.array([our_units]),\n",
    "                                 np.array([enemy_units]),\n",
    "                                 np.array([enemy_core]),\n",
    "                                 np.array([our_piles]),\n",
    "                                 np.array([our_hand])\n",
    "                       )\n",
    "print(qvalues)\n",
    "main = np.array([main, main])\n",
    "our_units = np.array([our_units, our_units])\n",
    "enemy_units = np.array([enemy_units, enemy_units])\n",
    "enemy_core = np.array([enemy_core, enemy_core])\n",
    "our_piles = np.array([our_piles, our_piles])\n",
    "our_hand = np.array([our_hand, our_hand])\n",
    "qvalues = actor_network(main, our_units, enemy_units, enemy_core, our_piles, our_hand)\n",
    "qvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(qvalues[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main, our_units, enemy_units, enemy_core, our_piles, our_hand = actor_network.parse_state(state)\n",
    "qvalues = actor_network(np.array([main]),\n",
    "                                 np.array([our_units]),\n",
    "                                 np.array([enemy_units]),\n",
    "                                 np.array([enemy_core]),\n",
    "                                 np.array([our_piles]),\n",
    "                                 np.array([our_hand])\n",
    "                       )\n",
    "def envActionFromAction(action, home_turn):\n",
    "    #skip\n",
    "    env_action = (\"skip\")\n",
    "    if action == 0:\n",
    "        return env_action\n",
    "    \n",
    "    #attack core\n",
    "    action -= 1\n",
    "    if(action < 7):\n",
    "        env_action = (\"attack\", [home_turn, action], [1 - home_turn, 0])\n",
    "        return env_action\n",
    "    \n",
    "    #attack unit\n",
    "    action -= 7\n",
    "    if(action < 49):\n",
    "        env_action = (\"attack\", [home_turn, action // 7], [1 - home_turn, action % 7])\n",
    "        return env_action\n",
    "    \n",
    "    #play from pile\n",
    "    action -= 49\n",
    "    if(action < 4):\n",
    "        env_action = (\"play\", action)\n",
    "        return env_action\n",
    "    \n",
    "    #play from hand\n",
    "    action -= 4\n",
    "    if(action < 6):\n",
    "        env_action = (\"play_hand\", action)\n",
    "        return env_action\n",
    "    \n",
    "    #move from pile to hand\n",
    "    action -= 6\n",
    "    env_action = (\"move\", action)\n",
    "    if action < 4:\n",
    "        return env_action\n",
    "    return None\n",
    "\n",
    "home_turn = 0\n",
    "valid_actions = [[]]\n",
    "actions = session.getValidActions()\n",
    "n_actions = 1 + 7 * 8 + 4 + 6 + 4\n",
    "for i in range(n_actions):\n",
    "    env_action = envActionFromAction(i, home_turn)\n",
    "    valid_actions[0].append(env_action in actions)\n",
    "\n",
    "print(valid_actions)\n",
    "#get valid action from network\n",
    "action = actor_network.sample_actions(qvalues.detach().numpy(), np.array(valid_actions))[0]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, MAIN_SIZE):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(MAIN_SIZE, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state_t):\n",
    "        reward = self.layers(state_t)\n",
    "        reward = reward.reshape(reward.shape[0])\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_network = ValueNetwork(MAIN_SIZE)\n",
    "value_network_optim = torch.optim.Adam(value_network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_reward(r, gamma, final_r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = final_r\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "def play_and_record(agent, session, n_steps = 1000, home_turn=0):\n",
    "    rewards = 0\n",
    "    \n",
    "    all_actions = []\n",
    "    for i in range(n_actions):\n",
    "        all_actions.append(envActionFromAction(i, home_turn))\n",
    "    \n",
    "    states_log = []\n",
    "    actions_log = []\n",
    "    rewards_log = []\n",
    "    final_state = None\n",
    "    final_r = 0\n",
    "    state = session.getObservation()\n",
    "    is_done = False\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        \n",
    "        table = session.getObservation()[\"battleGround\"][\"table\"]\n",
    "        hits_diff = table[home_turn][0][2] - table[1 - home_turn][0][2]\n",
    "        \n",
    "        state = createStateObservation(state)\n",
    "        \n",
    "        states_log.append(agent.parse_state(state))\n",
    "        #get actions\n",
    "        actions = session.getValidActions()\n",
    "        log_softmax_action = agent.get_qvalues_from_state(state)\n",
    "        softmax_action = torch.exp(log_softmax_action)\n",
    "        qvalues = softmax_action.data.cpu().numpy()\n",
    "        \n",
    "        valid_actions = [[]]\n",
    "        for i in all_actions:\n",
    "            valid_actions[0].append(i in actions)\n",
    "            \n",
    "            \n",
    "        #get valid action from network\n",
    "        action = agent.sample_actions(qvalues, np.array(valid_actions))[0]\n",
    "        one_hot_action = [int(k == action) for k in range(n_actions)]\n",
    "        actions_log.append(one_hot_action)\n",
    "        \n",
    "        #step\n",
    "        env_action = envActionFromAction(action, home_turn)\n",
    "        next_s = session.action(env_action)\n",
    "        \n",
    "        endGame = next_s[\"end\"]\n",
    "        \n",
    "        #enemy turn\n",
    "        if (next_s[\"turn\"] != home_turn) and not endGame:\n",
    "            actions = session.getValidActions()\n",
    "            while len(actions) > 1 and not endGame:  \n",
    "                next_s = session.action(random.choice(actions[1:]))\n",
    "                actions = session.getValidActions()\n",
    "                endGame = next_s[\"end\"]\n",
    "            if not endGame:\n",
    "                next_s = session.action(random.choice(actions[0]))\n",
    "        \n",
    "        #calc reward\n",
    "        table = next_s[\"battleGround\"][\"table\"]\n",
    "        reward = table[home_turn][0][2] - table[1 - home_turn][0][2] - hits_diff - 0.1\n",
    "        rewards_log.append(reward)\n",
    "        \n",
    "        final_state = next_s\n",
    "        \n",
    "        #morph game state to network state\n",
    "        \n",
    "        state = next_s\n",
    "        \n",
    "        if endGame:\n",
    "            is_done = True\n",
    "            session.reset()\n",
    "            break\n",
    "\n",
    "    if not is_done:\n",
    "        final_r = value_network(Variable(torch.Tensor([createStateObservation(final_state)[\"main\"]]))).cpu().data.numpy()\n",
    "\n",
    "    return states_log, actions_log, rewards_log, final_r, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_turn = 0\n",
    "states, actions, rewards, final_r, _ =  play_and_record(actor_network, session, n_steps=20, home_turn=0)\n",
    "print(rewards)\n",
    "actions_var = Variable(torch.Tensor(actions).view(-1, n_actions))\n",
    "\n",
    "# train actor network\n",
    "actor_network_optim.zero_grad()\n",
    "main = []\n",
    "our_units = []\n",
    "enemy_units = []\n",
    "enemy_core = []\n",
    "discount = 0.99\n",
    "our_piles = []\n",
    "our_hands = []\n",
    "for i in states:\n",
    "    main.append(i[0])\n",
    "    our_units.append(i[1])\n",
    "    enemy_units.append(i[2])\n",
    "    enemy_core.append(i[3])\n",
    "    our_piles.append(i[4])\n",
    "    our_hands.append(i[5])\n",
    "log_softmax_actions = actor_network(np.array(main), \n",
    "                                    np.array(our_units), \n",
    "                                    np.array(enemy_units), \n",
    "                                    np.array(enemy_core), \n",
    "                                    np.array(our_piles),\n",
    "                                    np.array(our_hand)\n",
    "                                   )\n",
    "\n",
    "states_var = Variable(torch.Tensor(np.array(main)).view(-1, MAIN_SIZE))\n",
    "vs = value_network(states_var).detach()\n",
    "\n",
    "# calculate qs\n",
    "qs = Variable(torch.Tensor(discount_reward(rewards, discount, final_r)))\n",
    "\n",
    "advantages = qs - vs\n",
    "actor_network_loss = -torch.mean(torch.sum(log_softmax_actions * actions_var, 1) * advantages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = session.reset()\n",
    "\n",
    "value_network = ValueNetwork(MAIN_SIZE)\n",
    "value_network_optim = torch.optim.Adam(value_network.parameters(), lr=0.01)\n",
    "\n",
    "actor_network = ActorNetwork(MAIN_SIZE, CARD_SIZE, CORE_SIZE, PILE_SIZE, HAND_SIZE, VEC_SIZE, epsilon = 0.1)\n",
    "actor_network_optim = torch.optim.Adam(actor_network.parameters(), lr = 0.01)\n",
    "\n",
    "discount = 0.99\n",
    "\n",
    "home_turn = 0\n",
    "\n",
    "mean_rewards = []\n",
    "mean_hits_diff = []\n",
    "mean_enemy_core_hits = []\n",
    "mean_win = []\n",
    "mean_loss_actor = []\n",
    "mean_loss_value = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "state = session.reset()\n",
    "batch_loss_actor = []\n",
    "batch_loss_value = []\n",
    "for i in trange(10000):\n",
    "    \n",
    "    \n",
    "    states, actions, rewards, final_r, _ =  play_and_record(actor_network, session, n_steps=200, home_turn=0)\n",
    "    \n",
    "    actions_var = Variable(torch.Tensor(actions).view(-1, n_actions))\n",
    "    \n",
    "\n",
    "    # train actor network\n",
    "    actor_network_optim.zero_grad()\n",
    "    main = []\n",
    "    our_units = []\n",
    "    enemy_units = []\n",
    "    enemy_core = []\n",
    "    discount = 0.99\n",
    "    our_piles = []\n",
    "    our_hands = []\n",
    "    for st in states:\n",
    "        main.append(st[0])\n",
    "        our_units.append(st[1])\n",
    "        enemy_units.append(st[2])\n",
    "        enemy_core.append(st[3])\n",
    "        our_piles.append(st[4])\n",
    "        our_hands.append(st[5])\n",
    "    log_softmax_actions = actor_network(np.array(main), \n",
    "                                        np.array(our_units), \n",
    "                                        np.array(enemy_units), \n",
    "                                        np.array(enemy_core), \n",
    "                                        np.array(our_piles),\n",
    "                                        np.array(our_hands)\n",
    "                                       )\n",
    "\n",
    "    states_var = Variable(torch.Tensor(np.array(main)).view(-1, MAIN_SIZE))\n",
    "    vs = value_network(states_var).detach()\n",
    "    \n",
    "    # calculate qs\n",
    "    qs = Variable(torch.Tensor(discount_reward(rewards, discount, final_r)))\n",
    "\n",
    "    advantages = qs - vs\n",
    "    actor_network_loss = -torch.mean(torch.sum(log_softmax_actions * actions_var, 1) * advantages)\n",
    "    batch_loss_actor.append(actor_network_loss.detach().numpy())\n",
    "    actor_network_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(actor_network.parameters(), 0.5)\n",
    "    actor_network_optim.step()\n",
    "\n",
    "    # train value network\n",
    "    value_network_optim.zero_grad()\n",
    "    target_values = qs\n",
    "    values = value_network(states_var)\n",
    "    criterion = nn.MSELoss()\n",
    "    value_network_loss = criterion(values, target_values)\n",
    "    batch_loss_value.append(value_network_loss.detach().numpy())\n",
    "    value_network_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(value_network.parameters(), 0.5)\n",
    "    value_network_optim.step()\n",
    "\n",
    "    # Testing\n",
    "    if (i + 1) % 50== 0:\n",
    "        actor_network.epsilon *= 0.99\n",
    "        \n",
    "        remembered_eps = actor_network.epsilon\n",
    "        \n",
    "        session.reset()\n",
    "        rewards_eval = []\n",
    "        hits_diff = []\n",
    "        wins = []\n",
    "        enemy_core_hits = []\n",
    "\n",
    "        eval_games = 20\n",
    "\n",
    "        for j in range(eval_games):\n",
    "            _, _, rewards_log, _, final_state = play_and_record(actor_network, session, n_steps=1000, home_turn=0)\n",
    "\n",
    "            table = final_state[\"battleGround\"][\"table\"]\n",
    "            hits_d = table[home_turn][0][2] - table[1 - home_turn][0][2]\n",
    "\n",
    "            enemy_core_hits.append(table[1 - home_turn][0][2])\n",
    "            rewards_eval.append(np.sum(rewards_log))\n",
    "            hits_diff.append(hits_d)\n",
    "            wins.append(final_state[\"loser\"] != home_turn)\n",
    "        \n",
    "        actor_network.epsilon = remembered_eps\n",
    "\n",
    "        mean_enemy_core_hits.append(np.mean(enemy_core_hits))\n",
    "        mean_hits_diff.append(np.mean(hits_diff))\n",
    "\n",
    "        mean_win.append(np.count_nonzero(np.array(wins) == True) / eval_games)\n",
    "\n",
    "        mean_rewards.append(np.mean(rewards_eval))\n",
    "        mean_loss_actor.append(np.mean(batch_loss_actor))\n",
    "        mean_loss_value.append(np.mean(batch_loss_value))\n",
    "\n",
    "        batch_loss_actor = []\n",
    "        batch_loss_value = []\n",
    "\n",
    "        log_dict = dict()\n",
    "        log_dict[\"mean_rewards\"] = mean_rewards\n",
    "        log_dict[\"mean_loss_actor\"] = mean_loss_actor\n",
    "        log_dict[\"mean_loss_value\"] = mean_loss_value\n",
    "        log_dict[\"mean_win\"] = mean_win\n",
    "        log_dict[\"mean_hits_diff\"] = mean_hits_diff\n",
    "\n",
    "        with open('log_a2c_random.pickle', 'wb') as f:\n",
    "            pickle.dump(log_dict, f)\n",
    "        \n",
    "        with open('model_a2c_random.pickle', 'wb') as f:\n",
    "            pickle.dump(actor_network, f)\n",
    "\n",
    "        clear_output(True)\n",
    "\n",
    "        plt.figure(figsize=[20, 8])    \n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.title(\"mean reward per game\")\n",
    "        plt.plot(range(len(mean_rewards)), mean_rewards)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.title(\"actor loss\")\n",
    "        plt.plot(range(len(mean_loss_actor)), mean_loss_actor)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.title(\"value loss\")\n",
    "        plt.plot(range(len(mean_loss_value)), mean_loss_value)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.title(\"hits diff\")\n",
    "        plt.plot(range(len(mean_hits_diff)), mean_hits_diff)\n",
    "        #plt.plot(range(len(mean_enemy_core_hits)), mean_enemy_core_hits)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.title(\"winrate\")\n",
    "        plt.plot(range(len(mean_win)), mean_win)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        plt.savefig(\"train.png\");\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'env')\n",
    "sys.path.insert(0, 'lib')\n",
    "\n",
    "import ccg\n",
    "from agents import ARagent, A2Cagent, SkipAgent\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "import utils\n",
    "seed = 12345644\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "cards = pd.read_csv('env/configue/cardsTable.csv')\n",
    "cardsList = [ccg.Minion(i) for i in cards.values.tolist()]\n",
    "\n",
    "cores = pd.read_csv('env/configue/cores.csv')\n",
    "coreList = [ccg.Core(i) for i in cores.values.tolist()]\n",
    "\n",
    "playersNum = 2\n",
    "piles = [[ccg.Pile(cardsList, 10) for _ in range(4)] for _ in range(playersNum)]\n",
    "cores = np.random.choice(coreList, 2)\n",
    "decks = [ccg.Deck(cores[i], piles[i], i) for i in range(playersNum)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = ccg.Session(decks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer\n",
    "from agents import ARagent, A2Cagent\n",
    "from nets import ActorNetwork, ValueNetwork\n",
    "from replays import FlatReplay, PrioritizedReplay\n",
    "\n",
    "actor = ActorNetwork(session.processObservation())\n",
    "value = ValueNetwork(session.processObservation())\n",
    "a2cAgent = A2Cagent(actor, value, 0, PrioritizedReplay(), epsilon = 0.5)\n",
    "arAgent = ARagent(1)\n",
    "skipAgent = SkipAgent(1)\n",
    "\n",
    "trainer = Trainer(session, [a2cAgent, arAgent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling buffer\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'gpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d30f04b06873>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Filling buffer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Buffer filled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\rl_ccg\\lib\\trainer.py\u001b[0m in \u001b[0;36mplayGame\u001b[1;34m(self, session, record, replay_id, evaluate)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mplayGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaySteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplay_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mplaySteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\rl_ccg\\lib\\trainer.py\u001b[0m in \u001b[0;36mplaySteps\u001b[1;34m(self, n_steps, session, record, replay_id, evaluate)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mn_steps\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurAgent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidActions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidActionsEnv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[0mn_observation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidActions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidActionsEnv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\rl_ccg\\lib\\agents.py\u001b[0m in \u001b[0;36mgetAction\u001b[1;34m(self, observation, validActions, validEnvActions, evaluate)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m#softmax_action = torch.exp(log_softmax_action)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m#qvalues = softmax_action.data.cpu().numpy()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mqvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_softmax_action\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalidActions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'gpu'"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "wins = dict()\n",
    "wins[-1] = 0\n",
    "wins[0] = 0\n",
    "wins[1] = 0\n",
    "\n",
    "health_adv_log = []\n",
    "actions_num_log = []\n",
    "turns_log = []\n",
    "wins_log = []\n",
    "\n",
    "actor_loss_log = []\n",
    "value_loss_log = []\n",
    "\n",
    "actor_loss_epoch = []\n",
    "value_loss_epoch = []\n",
    "\n",
    "print(\"Filling buffer\")\n",
    "for i in range(10):\n",
    "    trainer.playGame(record = True, evaluate = False)\n",
    "print(\"Buffer filled\")       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    trainer.playSteps(30, replay_id = i)\n",
    "    observation, _, _ = session.processNewStateInfo()\n",
    "    winner = -1\n",
    "    if(observation[\"loser\"] != -1):\n",
    "        winner = 1 - observation[\"loser\"]\n",
    "    wins[winner] += 1\n",
    "    \n",
    "    actor_loss, value_loss = trainer.train()[0]\n",
    "    actor_loss_epoch.append(actor_loss)\n",
    "    value_loss_epoch.append(value_loss)\n",
    "    \n",
    "    if(i % 50 == 0):\n",
    "        clear_output()\n",
    "        eval_games = 100\n",
    "        mean_health_adv = 0\n",
    "        mean_actions_num = 0\n",
    "        mean_turns = 0\n",
    "        mean_wins = 0\n",
    "        for i in range(eval_games):\n",
    "            adv, actions_num, turns, winner = trainer.playGame(record = False, evaluate = True)\n",
    "            mean_health_adv += adv[0]\n",
    "            mean_actions_num += actions_num[0]\n",
    "            mean_turns += turns\n",
    "            mean_wins += winner[0]\n",
    "            \n",
    "        health_adv_log.append(mean_health_adv / eval_games)\n",
    "        actions_num_log.append(mean_actions_num / eval_games)\n",
    "        turns_log.append(mean_turns / eval_games)\n",
    "        wins_log.append(mean_wins / eval_games)\n",
    "        \n",
    "        value_loss_log.append(np.mean(value_loss_epoch))\n",
    "        actor_loss_log.append(np.mean(actor_loss_epoch))\n",
    "        \n",
    "        acotor_loss_epoch = []\n",
    "        value_loss_epoch = []\n",
    "        \n",
    "        fig = plt.figure(figsize=(13, 8))\n",
    "\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.plot(range(len(health_adv_log)), health_adv_log)\n",
    "        plt.title(\"Health advantage\")\n",
    "        \n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.plot(range(len(actions_num_log)), actions_num_log)\n",
    "        plt.title(\"Actions num\")\n",
    "        \n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.plot(range(len(turns_log)), turns_log)\n",
    "        plt.title(\"Turns num\")\n",
    "        \n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.plot(range(len(wins_log)), wins_log)\n",
    "        plt.title(\"Win rate\")\n",
    "        \n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.plot(range(len(actor_loss_log)), actor_loss_log)\n",
    "        plt.title(\"Actor loss\")\n",
    "        \n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.plot(range(len(value_loss_log)), value_loss_log)\n",
    "        plt.title(\"Value loss\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
